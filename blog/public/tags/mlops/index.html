<!doctype html><html lang=zh-cn dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MLOps | æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·</title><meta name=keywords content><meta name=description content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"><meta name=author content="util.cn Team"><link rel=canonical href=/blog/tags/mlops/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.7e8505b7cdf8bb22ab2305e53c2700bb06c7e64faeb72cd3468823a9a3bd3d6e.css integrity="sha256-foUFt834uyKrIwXlPCcAuwbH5k+utyzTRogjqaO9PW4=" rel="preload stylesheet" as=style><link rel=icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=apple-touch-icon href=/blog/apple-touch-icon.png><link rel=mask-icon href=/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=/blog/tags/mlops/feed.xml title=rss><link rel=alternate hreflang=zh-cn href=/blog/tags/mlops/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script src=/js/external-link-config.js></script><script src=/js/external-link-interceptor.js></script><link rel=stylesheet href=/blog/css/custom.css media=screen><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","name":"æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«","url":"https://www.util.cn/blog/","description":"æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚æä¾›JSONæ ¼å¼åŒ–ã€SQLä¼˜åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰åœ¨çº¿å·¥å…·çš„è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼Œå¸®åŠ©å¼€å‘è€…æå‡å·¥ä½œæ•ˆç‡ã€‚","publisher":{"@type":"Organization","name":"æœ‰æ¡å·¥å…·","url":"https://www.util.cn","logo":{"@type":"ImageObject","url":"https://www.util.cn/blog/logo/logo-256.png","width":256,"height":256}},"potentialAction":[{"@type":"SearchAction","target":"https://www.util.cn/blog/search?q={search_term_string}","query-input":"required name=search_term_string"}]}</script><meta property="og:type" content="website"><meta property="og:title" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«"><meta property="og:description" content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚æä¾›JSONæ ¼å¼åŒ–ã€SQLä¼˜åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰åœ¨çº¿å·¥å…·çš„è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼Œå¸®åŠ©å¼€å‘è€…æå‡å·¥ä½œæ•ˆç‡ã€‚"><meta property="og:url" content="https://www.util.cn/blog/"><meta property="og:site_name" content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢"><meta property="og:image" content="https://www.util.cn/blog/logo/logo-256.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«"><meta name=twitter:description content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚"><meta name=twitter:image content="https://www.util.cn/blog/logo/logo-256.png"><meta name=baidu-site-verification content><meta name=category content="æŠ€æœ¯åšå®¢,å¼€å‘è€…å·¥å…·,ç¼–ç¨‹æ•™ç¨‹"><meta name=coverage content="Worldwide"><meta name=distribution content="Global"><meta name=rating content="General"><script id=51la_code async crossorigin=anonymous src="https://sdk.51.la/js-sdk-pro.min.js?id=3OM52V0xJAPv6ozF&hash=pro"></script><script>window.addEventListener("load",function(){setTimeout(function(){typeof la!="undefined"?console.log("51laç»Ÿè®¡å·²åŠ è½½"):(console.warn("51laç»Ÿè®¡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ"),function(){var e=document.createElement("script");e.src="https://sdk.51.la/js-sdk-pro.min.js?id=3OM52V0xJAPv6ozF&hash=backup",e.async=!0,document.head.appendChild(e)}())},3e3)})</script><meta property="og:url" content="/blog/tags/mlops/"><meta property="og:site_name" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·"><meta property="og:title" content="MLOps"><meta property="og:description" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="MLOps"><meta name=twitter:description content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=/blog/ accesskey=h title="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· (Alt + H)">æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/blog/posts/ title="æ‰€æœ‰æ–‡ç« åˆ—è¡¨ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŸ¥çœ‹æ‰€æœ‰æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹"><span>æ–‡ç« </span></a></li><li><a href=https://www.util.cn title="æœ‰æ¡å·¥å…· - å¼€å‘è€…çš„å¸¸ç”¨å·¥å…·é›†åˆï¼šæ— å¹¿å‘Š Â· æœ¬åœ°è®¡ç®— Â· å³å¼€å³ç”¨çš„åœ¨çº¿å·¥å…·å¹³å°ï¼Œæä¾›JSONæ ¼å¼åŒ–ã€SQLæ ¼å¼åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰å®ç”¨å·¥å…·"><span>æœ‰æ¡å·¥å…·</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=/blog/categories/ title="æ–‡ç« åˆ†ç±» - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŒ‰æŠ€æœ¯é¢†åŸŸåˆ†ç±»çš„ä¼˜è´¨æ–‡ç« ï¼ŒåŒ…æ‹¬å‰ç«¯å¼€å‘ã€å·¥å…·ä½¿ç”¨ã€ç¼–ç¨‹æŠ€å·§ç­‰"><span>åˆ†ç±»</span></a></li><li><a href=/blog/tags/ title="æ ‡ç­¾äº‘ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šé€šè¿‡æ ‡ç­¾å¿«é€Ÿæ‰¾åˆ°æ„Ÿå…´è¶£çš„æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹"><span>æ ‡ç­¾</span></a></li><li><a href=/blog/archives/ title="æ–‡ç« å½’æ¡£ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŒ‰æ—¶é—´æŸ¥çœ‹å†å²æ–‡ç« "><span>å½’æ¡£</span></a></li><li><a href=/blog/search/ title="æœç´¢æ–‡ç«  - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šé€šè¿‡å…³é”®è¯æœç´¢æ‰¾åˆ°æ„Ÿå…´è¶£çš„æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹ï¼Œæ”¯æŒæ ‡é¢˜ã€å†…å®¹ã€åˆ†ç±»å’Œæ ‡ç­¾æœç´¢"><span>æœç´¢</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>MLOps</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>AIä¸æœºå™¨å­¦ä¹ å·¥ç¨‹åŒ–å®è·µï¼šä»æ¨¡å‹åˆ°ç”Ÿäº§ç³»ç»Ÿçš„å®Œæ•´æŒ‡å—</h2></header><div class=entry-content><p>å¼•è¨€ å°†AI/MLæ¨¡å‹ä»ç ”ç©¶ç¯å¢ƒæ¨å‘ç”Ÿäº§ç¯å¢ƒæ˜¯ä¸€é¡¹å¤æ‚çš„å·¥ç¨‹æŒ‘æˆ˜ã€‚é™¤äº†æ¨¡å‹æœ¬èº«çš„å‡†ç¡®æ€§ï¼Œè¿˜éœ€è¦è€ƒè™‘å¯æ‰©å±•æ€§ã€å¯é æ€§ã€å¯ç»´æŠ¤æ€§ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨AI/MLç³»ç»Ÿçš„å·¥ç¨‹åŒ–å®è·µï¼Œå¸®åŠ©å›¢é˜Ÿæ„å»ºç¨³å®šé«˜æ•ˆçš„ç”Ÿäº§çº§AIç³»ç»Ÿã€‚
ä¸€ã€MLOpsæ¦‚è¿° 1.1 MLOpsçš„æ ¸å¿ƒç»„ä»¶ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 # ========== MLOpsæ¶æ„æ¦‚è§ˆ ========== """ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ æ•°æ®å±‚ â”‚ â”‚ - åŸå§‹æ•°æ® â”‚ â”‚ - ç‰¹å¾å­˜å‚¨ â”‚ â”‚ - è®­ç»ƒæ•°æ® â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ è®­ç»ƒå±‚ â”‚ â”‚ - ç‰¹å¾å·¥ç¨‹ â”‚ â”‚ - æ¨¡å‹è®­ç»ƒ â”‚ â”‚ - è¶…å‚æ•°è°ƒä¼˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ è¯„ä¼°å±‚ â”‚ â”‚ - æ¨¡å‹è¯„ä¼° â”‚ â”‚ - A/Bæµ‹è¯• â”‚ â”‚ - æ¨¡å‹éªŒè¯ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ éƒ¨ç½²å±‚ â”‚ â”‚ - æ¨¡å‹æœåŠ¡ â”‚ â”‚ - æ‰¹é‡æ¨ç† â”‚ â”‚ - è¾¹ç¼˜éƒ¨ç½² â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ç›‘æ§å±‚ â”‚ â”‚ - æ€§èƒ½ç›‘æ§ â”‚ â”‚ - æ•°æ®æ¼‚ç§»æ£€æµ‹ â”‚ â”‚ - å‘Šè­¦ç³»ç»Ÿ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ """ # MLOpså„é˜¶æ®µçš„å…³é”®ä»»åŠ¡ MLOPS_PIPELINE = { "data": { "ingestion": "æ•°æ®é‡‡é›†ä¸æ¸…æ´—", "validation": "æ•°æ®è´¨é‡æ£€æŸ¥", "feature_engineering": "ç‰¹å¾æå–ä¸è½¬æ¢", "feature_store": "ç‰¹å¾å­˜å‚¨ä¸ç‰ˆæœ¬ç®¡ç†" }, "training": { "experiment_tracking": "å®éªŒè¿½è¸ª", "hyperparameter_tuning": "è¶…å‚æ•°ä¼˜åŒ–", "model_training": "æ¨¡å‹è®­ç»ƒ", "model_evaluation": "æ¨¡å‹è¯„ä¼°" }, "deployment": { "model_serving": "æ¨¡å‹æœåŠ¡åŒ–", "canary_deployment": "é‡‘ä¸é›€éƒ¨ç½²", "model_versioning": "æ¨¡å‹ç‰ˆæœ¬ç®¡ç†", "rollback": "å›æ»šæœºåˆ¶" }, "monitoring": { "performance_monitoring": "æ€§èƒ½ç›‘æ§", "data_drift_detection": "æ•°æ®æ¼‚ç§»æ£€æµ‹", "model_explainability": "æ¨¡å‹å¯è§£é‡Šæ€§", "alerting": "å‘Šè­¦ç³»ç»Ÿ" } } 1.2 å®éªŒç®¡ç†ä¸è¿½è¸ª 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 # ========== å®éªŒè¿½è¸ªç³»ç»Ÿ ========== import mlflow import mlflow.sklearn from datetime import datetime from typing import Any, Dict, Optional class ExperimentTracker: """å®éªŒè¿½è¸ªå™¨""" def __init__(self, tracking_uri: str, experiment_name: str): mlflow.set_tracking_uri(tracking_uri) mlflow.set_experiment(experiment_name) self.experiment_name = experiment_name def start_run(self, run_name: Optional[str] = None): """å¼€å§‹ä¸€æ¬¡è¿è¡Œ""" self.run = mlflow.start_run(run_name=run_name) return self.run def log_params(self, params: Dict[str, Any]): """è®°å½•å‚æ•°""" mlflow.log_params(params) def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None): """è®°å½•æŒ‡æ ‡""" mlflow.log_metrics(metrics, step=step) def log_model(self, model: Any, artifact_path: str = "model"): """è®°å½•æ¨¡å‹""" mlflow.sklearn.log_model(model, artifact_path) def log_artifact(self, file_path: str): """è®°å½•æ–‡ä»¶""" mlflow.log_artifact(file_path) def log_figure(self, figure, artifact_file: str): """è®°å½•å›¾è¡¨""" mlflow.log_figure(figure, artifact_file) def end_run(self, status: str = "FINISHED"): """ç»“æŸè¿è¡Œ""" mlflow.end_run(status=status) # ä½¿ç”¨ç¤ºä¾‹ def train_model_with_tracking(X_train, y_train, X_test, y_test, params): """è®­ç»ƒæ¨¡å‹å¹¶è¿½è¸ªå®éªŒ""" tracker = ExperimentTracker( tracking_uri="http://mlflow-server:5000", experiment_name="fraud-detection" ) tracker.start_run(run_name=f"experiment-{datetime.now().strftime('%Y%m%d-%H%M%S')}") try: # è®°å½•å‚æ•° tracker.log_params(params) # è®­ç»ƒæ¨¡å‹ model = train_model(X_train, y_train, params) # è¯„ä¼°æ¨¡å‹ metrics = evaluate_model(model, X_test, y_test) tracker.log_metrics(metrics) # è®°å½•æ¨¡å‹ tracker.log_model(model) # è®°å½•å­¦ä¹ æ›²çº¿ fig = plot_learning_curve(model, X_train, y_train) tracker.log_figure(fig, "learning_curve.png") # è®°å½•ç‰¹å¾é‡è¦æ€§ fig = plot_feature_importance(model) tracker.log_figure(fig, "feature_importance.png") tracker.end_run(status="FINISHED") return model, metrics except Exception as e: tracker.end_run(status="FAILED") raise e # ========== è¶…å‚æ•°ä¼˜åŒ– ========== import optuna from optuna.integration.mlflow import MLflowCallback class HyperparameterOptimizer: """è¶…å‚æ•°ä¼˜åŒ–å™¨""" def __init__(self, n_trials: int = 100, timeout: Optional[int] = None): self.n_trials = n_trials self.timeout = timeout self.study = None def objective(self, trial, X_train, y_train, X_val, y_val): """ä¼˜åŒ–ç›®æ ‡å‡½æ•°""" # å®šä¹‰æœç´¢ç©ºé—´ params = { 'n_estimators': trial.suggest_int('n_estimators', 50, 500), 'max_depth': trial.suggest_int('max_depth', 3, 20), 'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True), 'subsample': trial.suggest_float('subsample', 0.5, 1.0), 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), 'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), } # è®­ç»ƒæ¨¡å‹ model = train_model(X_train, y_train, params) # è¯„ä¼° predictions = model.predict(X_val) score = calculate_metric(y_val, predictions) return score def optimize(self, X_train, y_train, X_val, y_val): """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–""" # åˆ›å»ºç ”ç©¶å¯¹è±¡ self.study = optuna.create_study( direction="maximize", study_name="hyperparameter-optimization" ) # æ·»åŠ MLflowå›è°ƒ mlflc = MLflowCallback( tracking_uri="http://mlflow-server:5000", metric_name="validation_score" ) # æ‰§è¡Œä¼˜åŒ– self.study.optimize( lambda trial: self.objective(trial, X_train, y_train, X_val, y_val), n_trials=self.n_trials, timeout=self.timeout, callbacks=[mlflc] ) return self.study.best_params, self.study.best_value def get_importance(self): """è·å–è¶…å‚æ•°é‡è¦æ€§""" return optuna.importance.get_param_importances(self.study) äºŒã€ç‰¹å¾å·¥ç¨‹ä¸ç®¡ç† 2.1 ç‰¹å¾å­˜å‚¨æ¶æ„ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 # ========== ç‰¹å¾å­˜å‚¨ç³»ç»Ÿ ========== from abc import ABC, abstractmethod from typing import List, Dict, Any import pandas as pd from datetime import datetime, timedelta class FeatureStore(ABC): """ç‰¹å¾å­˜å‚¨æŠ½è±¡ç±»""" @abstractmethod def get_features(self, entity_ids: List[str], feature_names: List[str]) -> pd.DataFrame: """è·å–ç‰¹å¾""" pass @abstractmethod def write_features(self, entity_id: str, features: Dict[str, Any]): """å†™å…¥ç‰¹å¾""" pass class OfflineFeatureStore(FeatureStore): """ç¦»çº¿ç‰¹å¾å­˜å‚¨ - ç”¨äºè®­ç»ƒ""" def __init__(self, storage_path: str): self.storage_path = storage_path def get_features(self, entity_ids: List[str], feature_names: List[str]) -> pd.DataFrame: """ä»å­˜å‚¨è·å–ç‰¹å¾""" # ä»Parquetæ–‡ä»¶è¯»å– df = pd.read_parquet(f"{self.storage_path}/features.parquet") # è¿‡æ»¤å®ä½“ df = df[df['entity_id'].isin(entity_ids)] # é€‰æ‹©ç‰¹å¾åˆ— return df[['entity_id'] + feature_names] def write_features(self, entity_id: str, features: Dict[str, Any]): """å†™å…¥ç‰¹å¾åˆ°å­˜å‚¨""" # å®ç°å†™å…¥é€»è¾‘ pass def create_training_set( self, entity_ids: List[str], feature_names: List[str], label_name: str ) -> pd.DataFrame: """åˆ›å»ºè®­ç»ƒæ•°æ®é›†""" df = self.get_features(entity_ids, feature_names + [label_name]) return df class OnlineFeatureStore(FeatureStore): """åœ¨çº¿ç‰¹å¾å­˜å‚¨ - ç”¨äºæ¨ç†""" def __init__(self, redis_client): self.redis = redis_client def get_features(self, entity_ids: List[str], feature_names: List[str]) -> pd.DataFrame: """ä»Redisè·å–å®æ—¶ç‰¹å¾""" features = [] for entity_id in entity_ids: key = f"feature:{entity_id}" data = self.redis.hgetall(key) feature_dict = { 'entity_id': entity_id } for feature_name in feature_names: feature_dict[feature_name] = data.get(feature_name) features.append(feature_dict) return pd.DataFrame(features) def write_features(self, entity_id: str, features: Dict[str, Any]): """å†™å…¥ç‰¹å¾åˆ°Redis""" key = f"feature:{entity_id}" # æ·»åŠ æ—¶é—´æˆ³ features['updated_at'] = datetime.now().isoformat() self.redis.hset(key, mapping=features) # è®¾ç½®è¿‡æœŸæ—¶é—´ self.redis.expire(key, timedelta(days=7)) class FeatureEngineeringPipeline: """ç‰¹å¾å·¥ç¨‹ç®¡é“""" def __init__(self, config: Dict[str, Any]): self.config = config self.transformers = {} def fit(self, df: pd.DataFrame): """æ‹Ÿåˆå˜æ¢å™¨""" for feature_config in self.config['features']: feature_name = feature_config['name'] transform_type = feature_config['transform'] if transform_type == 'standard': from sklearn.preprocessing import StandardScaler transformer = StandardScaler() transformer.fit(df[[feature_name]]) self.transformers[feature_name] = transformer elif transform_type == 'minmax': from sklearn.preprocessing import MinMaxScaler transformer = MinMaxScaler() transformer.fit(df[[feature_name]]) self.transformers[feature_name] = transformer elif transform_type == 'label': from sklearn.preprocessing import LabelEncoder transformer = LabelEncoder() transformer.fit(df[feature_name]) self.transformers[feature_name] = transformer def transform(self, df: pd.DataFrame) -> pd.DataFrame: """å˜æ¢æ•°æ®""" result_df = df.copy() for feature_name, transformer in self.transformers.items(): if isinstance(transformer, (StandardScaler, MinMaxScaler)): result_df[feature_name] = transformer.transform(df[[feature_name]]).flatten() elif isinstance(transformer, LabelEncoder): result_df[feature_name] = transformer.transform(df[feature_name]) return result_df def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame: """æ‹Ÿåˆå¹¶å˜æ¢""" self.fit(df) return self.transform(df) # ä½¿ç”¨ç¤ºä¾‹ def create_training_features(): """åˆ›å»ºè®­ç»ƒç‰¹å¾""" # åˆå§‹åŒ–ç¦»çº¿ç‰¹å¾å­˜å‚¨ offline_store = OfflineFeatureStore('/data/features') # è·å–åŸå§‹æ•°æ® raw_data = load_raw_data() # ç‰¹å¾å·¥ç¨‹ pipeline = FeatureEngineeringPipeline({ 'features': [ {'name': 'age', 'transform': 'standard'}, {'name': 'income', 'transform': 'minmax'}, {'name': 'category', 'transform': 'label'} ] }) # æ‹Ÿåˆå¹¶å˜æ¢ features = pipeline.fit_transform(raw_data) # å†™å…¥ç‰¹å¾å­˜å‚¨ for _, row in features.iterrows(): offline_store.write_features( row['entity_id'], row.to_dict() ) return features def get_online_features(entity_id: str): """è·å–åœ¨çº¿ç‰¹å¾""" import redis r = redis.Redis(host='localhost', port=6379) online_store = OnlineFeatureStore(r) features = online_store.get_features( [entity_id], ['age', 'income', 'category'] ) return features.iloc[0].to_dict() 2.2 ç‰¹å¾ç‰ˆæœ¬ç®¡ç† 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 # ========== ç‰¹å¾ç‰ˆæœ¬ç®¡ç† ========== class FeatureVersion: """ç‰¹å¾ç‰ˆæœ¬""" def __init__( self, feature_name: str, version: int, computation_logic: str, created_at: datetime ): self.feature_name = feature_name self.version = version self.computation_logic = computation_logic self.created_at = created_at class FeatureRegistry: """ç‰¹å¾æ³¨å†Œè¡¨""" def __init__(self): self.features = {} def register_feature( self, feature_name: str, computation_logic: str, description: str = "", owner: str = "" ): """æ³¨å†Œæ–°ç‰¹å¾""" if feature_name in self.features: # åˆ›å»ºæ–°ç‰ˆæœ¬ last_version = max(self.features[feature_name].keys()) new_version = last_version + 1 else: self.features[feature_name] = {} new_version = 1 feature_version = FeatureVersion( feature_name=feature_name, version=new_version, computation_logic=computation_logic, created_at=datetime.now() ) self.features[feature_name][new_version] = feature_version return new_version def get_feature(self, feature_name: str, version: Optional[int] = None): """è·å–ç‰¹å¾å®šä¹‰""" if feature_name not in self.features: raise ValueError(f"Feature {feature_name} not found") if version is None: # è·å–æœ€æ–°ç‰ˆæœ¬ version = max(self.features[feature_name].keys()) return self.features[feature_name][version] def list_features(self): """åˆ—å‡ºæ‰€æœ‰ç‰¹å¾""" return { name: max(versions.keys()) for name, versions in self.features.items() } # ä½¿ç”¨ç¤ºä¾‹ registry = FeatureRegistry() # æ³¨å†Œç‰¹å¾ registry.register_feature( feature_name="user_avg_transaction_amount", computation_logic=""" SELECT user_id, AVG(amount) as user_avg_transaction_amount FROM transactions WHERE transaction_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY) GROUP BY user_id """, description="ç”¨æˆ·è¿‡å»30å¤©å¹³å‡äº¤æ˜“é‡‘é¢", owner="data-team" ) # æ›´æ–°ç‰¹å¾é€»è¾‘ï¼ˆåˆ›å»ºæ–°ç‰ˆæœ¬ï¼‰ registry.register_feature( feature_name="user_avg_transaction_amount", computation_logic=""" SELECT user_id, AVG(amount) as user_avg_transaction_amount FROM transactions WHERE transaction_date >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY) AND status = 'completed' GROUP BY user_id """, description="ç”¨æˆ·è¿‡å»30å¤©å·²å®Œæˆäº¤æ˜“å¹³å‡é‡‘é¢", owner="data-team" ) ä¸‰ã€æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ– 3.1 æ¨¡å‹æœåŠ¡åŒ– 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 # ========== æ¨¡å‹æœåŠ¡ ========== from fastapi import FastAPI, HTTPException from pydantic import BaseModel from typing import List import joblib import numpy as np app = FastAPI(title="ML Model Service") class PredictionRequest(BaseModel): features: List[float] class PredictionResponse(BaseModel): prediction: float probability: float model_version: str timestamp: str class ModelService: """æ¨¡å‹æœåŠ¡""" def __init__(self, model_path: str): self.model = self.load_model(model_path) self.model_version = self.get_model_version(model_path) def load_model(self, model_path: str): """åŠ è½½æ¨¡å‹""" return joblib.load(model_path) def get_model_version(self, model_path: str) -> str: """è·å–æ¨¡å‹ç‰ˆæœ¬""" # ä»è·¯å¾„æˆ–å…ƒæ•°æ®ä¸­æå–ç‰ˆæœ¬ return model_path.split('/')[-1].replace('.pkl', '') def predict(self, features: List[float]) -> dict: """é¢„æµ‹""" X = np.array(features).reshape(1, -1) prediction = self.model.predict(X)[0] probability = self.model.predict_proba(X)[0].max() return { 'prediction': float(prediction), 'probability': float(probability) } # å…¨å±€æ¨¡å‹æœåŠ¡å®ä¾‹ model_service = ModelService("/models/fraud_detection_v1.pkl") @app.post("/predict", response_model=PredictionResponse) async def predict(request: PredictionRequest): """é¢„æµ‹æ¥å£""" try: result = model_service.predict(request.features) return PredictionResponse( prediction=result['prediction'], probability=result['probability'], model_version=model_service.model_version, timestamp=datetime.now().isoformat() ) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @app.get("/model/info") async def model_info(): """æ¨¡å‹ä¿¡æ¯æ¥å£""" return { "model_version": model_service.model_version, "model_type": type(model_service.model).__name__, "loaded_at": datetime.now().isoformat() } @app.get("/health") async def health_check(): """å¥åº·æ£€æŸ¥""" return {"status": "healthy"} # ========== æ‰¹é‡é¢„æµ‹æœåŠ¡ ========== class BatchPredictionService: """æ‰¹é‡é¢„æµ‹æœåŠ¡""" def __init__(self, model_path: str): self.model = joblib.load(model_path) self.batch_size = 1000 def predict_batch(self, features: List[List[float]]) -> List[dict]: """æ‰¹é‡é¢„æµ‹""" results = [] for i in range(0, len(features), self.batch_size): batch = features[i:i + self.batch_size] X = np.array(batch) predictions = self.model.predict(X) probabilities = self.model.predict_proba(X).max(axis=1) for pred, prob in zip(predictions, probabilities): results.append({ 'prediction': int(pred), 'probability': float(prob) }) return results @app.post("/predict/batch") async def predict_batch(request: PredictionRequest): """æ‰¹é‡é¢„æµ‹æ¥å£""" batch_service = BatchPredictionService("/models/fraud_detection_v1.pkl") results = batch_service.predict_batch([request.features]) return {"predictions": results} 3.2 æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä¸å›æ»š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 # ========== æ¨¡å‹ç‰ˆæœ¬ç®¡ç† ========== class ModelVersion: """æ¨¡å‹ç‰ˆæœ¬""" def __init__( self, version: str, model_path: str, metrics: Dict[str, float], created_at: datetime ): self.version = version self.model_path = model_path self.metrics = metrics self.created_at = created_at class ModelRegistry: """æ¨¡å‹æ³¨å†Œè¡¨""" def __init__(self, storage_path: str): self.storage_path = storage_path self.models = {} self.current_version = None def register_model( self, version: str, model_path: str, metrics: Dict[str, float] ): """æ³¨å†Œæ¨¡å‹""" model_version = ModelVersion( version=version, model_path=model_path, metrics=metrics, created_at=datetime.now() ) self.models[version] = model_version return model_version def set_current_version(self, version: str): """è®¾ç½®å½“å‰ç‰ˆæœ¬""" if version not in self.models: raise ValueError(f"Version {version} not found") self.current_version = version def get_current_model(self): """è·å–å½“å‰æ¨¡å‹""" if self.current_version is None: raise ValueError("No current version set") return self.models[self.current_version] def rollback(self, target_version: str): """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬""" if target_version not in self.models: raise ValueError(f"Version {target_version} not found") old_version = self.current_version self.current_version = target_version print(f"Rollback from {old_version} to {target_version}") def list_versions(self): """åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬""" return sorted( self.models.keys(), key=lambda v: self.models[v].created_at, reverse=True ) def compare_versions(self, version1: str, version2: str) -> dict: """æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬""" if version1 not in self.models or version2 not in self.models: raise ValueError("One or both versions not found") return { 'version1': { 'version': version1, 'metrics': self.models[version1].metrics }, 'version2': { 'version': version2, 'metrics': self.models[version2].metrics }, 'improvement': { metric: self.models[version2].metrics[metric] - self.models[version1].metrics[metric] for metric in self.models[version1].metrics } } # ========== ç°åº¦å‘å¸ƒ ========== class CanaryDeployment: """ç°åº¦éƒ¨ç½²ç®¡ç†""" def __init__(self, registry: ModelRegistry): self.registry = registry self.traffic_split = {} def set_traffic_split(self, version_percentages: Dict[str, float]): """è®¾ç½®æµé‡åˆ†é…""" total = sum(version_percentages.values()) if abs(total - 1.0) > 0.01: raise ValueError("Percentages must sum to 1.0") for version in version_percentages.keys(): if version not in self.registry.models: raise ValueError(f"Version {version} not found") self.traffic_split = version_percentages def route_request(self) -> str: """è·¯ç”±è¯·æ±‚åˆ°æŒ‡å®šç‰ˆæœ¬""" import random rand = random.random() cumulative = 0.0 for version, percentage in self.traffic_split.items(): cumulative += percentage if rand &lt;= cumulative: return version return self.registry.current_version def gradual_rollout( self, new_version: str, steps: int = 10, duration_hours: int = 24 ): """æ¸è¿›å¼ç°åº¦å‘å¸ƒ""" import asyncio step_duration = duration_hours * 3600 / steps async def rollout_step(step: int): percentage = (step + 1) / steps self.set_traffic_split({ new_version: percentage, self.registry.current_version: 1 - percentage }) print(f"Step {step + 1}/{steps}: {new_version} at {percentage:.1%}") await asyncio.sleep(step_duration) # æ‰§è¡Œæ¸è¿›å¼å‘å¸ƒ for step in range(steps): asyncio.run(rollout_step(step)) # å®Œå…¨åˆ‡æ¢åˆ°æ–°ç‰ˆæœ¬ self.registry.set_current_version(new_version) self.traffic_split = {new_version: 1.0} å››ã€æ¨¡å‹ç›‘æ§ä¸A/Bæµ‹è¯• 4.1 æ¨¡å‹æ€§èƒ½ç›‘æ§ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 # ========== æ¨¡å‹ç›‘æ§ç³»ç»Ÿ ========== from prometheus_client import Counter, Histogram, Gauge import numpy as np # å®šä¹‰ç›‘æ§æŒ‡æ ‡ prediction_count = Counter( 'ml_predictions_total', 'Total predictions made', ['model_version', 'prediction'] ) prediction_latency = Histogram( 'ml_prediction_duration_seconds', 'Prediction latency', ['model_version'] ) prediction_drift = Gauge( 'ml_prediction_distribution', 'Prediction distribution', ['model_version', 'prediction_class'] ) class ModelMonitor: """æ¨¡å‹ç›‘æ§å™¨""" def __init__(self, model_version: str, expected_distribution: dict): self.model_version = model_version self.expected_distribution = expected_distribution self.actual_predictions = [] def log_prediction( self, prediction: int, probability: float, latency: float ): """è®°å½•é¢„æµ‹""" prediction_count.labels( model_version=self.model_version, prediction=str(prediction) ).inc() prediction_latency.labels( model_version=self.model_version ).observe(latency) self.actual_predictions.append(prediction) def check_drift(self, threshold: float = 0.1) -> bool: """æ£€æŸ¥æ¼‚ç§»""" if len(self.actual_predictions) &lt; 100: return False # è®¡ç®—å®é™…åˆ†å¸ƒ actual_dist = {} for pred in self.actual_predictions: actual_dist[pred] = actual_dist.get(pred, 0) + 1 for key in actual_dist: actual_dist[key] /= len(self.actual_predictions) # è®¡ç®—åˆ†å¸ƒå·®å¼‚ drift_score = 0.0 for key in self.expected_distribution: expected = self.expected_distribution.get(key, 0) actual = actual_dist.get(key, 0) drift_score += abs(expected - actual) return drift_score > threshold def update_distribution(self): """æ›´æ–°æœŸæœ›åˆ†å¸ƒ""" if len(self.actual_predictions) &lt; 100: return new_dist = {} for pred in self.actual_predictions: new_dist[pred] = new_dist.get(pred, 0) + 1 for key in new_dist: new_dist[key] /= len(self.actual_predictions) self.expected_distribution = new_dist self.actual_predictions = [] class DataDriftDetector: """æ•°æ®æ¼‚ç§»æ£€æµ‹å™¨""" def __init__(self, reference_data: np.ndarray): self.reference_data = reference_data self.reference_mean = np.mean(reference_data, axis=0) self.reference_std = np.std(reference_data, axis=0) def detect_drift( self, current_data: np.ndarray, threshold: float = 3.0 ) -> dict: """æ£€æµ‹æ•°æ®æ¼‚ç§»""" current_mean = np.mean(current_data, axis=0) current_std = np.std(current_data, axis=0) # Z-scoreæ£€æµ‹ z_scores = np.abs( (current_mean - self.reference_mean) / self.reference_std ) drifted_features = np.where(z_scores > threshold)[0] return { 'drift_detected': len(drifted_features) > 0, 'drifted_features': drifted_features.tolist(), 'z_scores': z_scores.tolist() } # ä½¿ç”¨ç¤ºä¾‹ def create_model_monitor(): """åˆ›å»ºæ¨¡å‹ç›‘æ§å™¨""" # æœŸæœ›åˆ†å¸ƒï¼ˆä»è®­ç»ƒæ•°æ®è·å–ï¼‰ expected_dist = { 0: 0.95, # 95% æ­£å¸¸ 1: 0.05 # 5% æ¬ºè¯ˆ } monitor = ModelMonitor( model_version="v1.0", expected_distribution=expected_dist ) return monitor async def monitor_predictions(): """ç›‘æ§é¢„æµ‹""" monitor = create_model_monitor() while True: # è·å–é¢„æµ‹ç»“æœ predictions = await get_recent_predictions() for pred in predictions: monitor.log_prediction( prediction=pred['label'], probability=pred['probability'], latency=pred['latency'] ) # æ£€æŸ¥æ¼‚ç§» if monitor.check_drift(): send_alert("Prediction drift detected!") await asyncio.sleep(60) # æ¯åˆ†é’Ÿæ£€æŸ¥ 4.2 A/Bæµ‹è¯•æ¡†æ¶ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 # ========== A/Bæµ‹è¯•æ¡†æ¶ ========== class ABTest: """A/Bæµ‹è¯•""" def __init__( self, name: str, variants: List[str], traffic_split: Dict[str, float], metrics: List[str] ): self.name = name self.variants = variants self.traffic_split = traffic_split self.metrics = metrics self.results = {variant: {metric: [] for metric in metrics} for variant in variants} def assign_variant(self, user_id: str) -> str: """åˆ†é…ç”¨æˆ·åˆ°å˜ä½“""" import hashlib # ä½¿ç”¨ç”¨æˆ·IDçš„å“ˆå¸Œå€¼ä¿è¯ä¸€è‡´æ€§ hash_value = int(hashlib.md5(f"{self.name}:{user_id}".encode()).hexdigest(), 16) normalized = hash_value / (2 ** 32 - 1) cumulative = 0.0 for variant, percentage in self.traffic_split.items(): cumulative += percentage if normalized &lt;= cumulative: return variant return self.variants[-1] def record_metric(self, variant: str, metric: str, value: float): """è®°å½•æŒ‡æ ‡""" if variant not in self.results: raise ValueError(f"Unknown variant: {variant}") if metric not in self.metrics: raise ValueError(f"Unknown metric: {metric}") self.results[variant][metric].append(value) def analyze(self) -> dict: """åˆ†æA/Bæµ‹è¯•ç»“æœ""" from scipy import stats analysis = {} for metric in self.metrics: metric_analysis = {} # è®¡ç®—æ¯ä¸ªå˜ä½“çš„ç»Ÿè®¡ä¿¡æ¯ for variant in self.variants: values = self.results[variant][metric] if len(values) == 0: continue metric_analysis[variant] = { 'mean': np.mean(values), 'std': np.std(values), 'count': len(values) } # æ¯”è¾ƒå˜ä½“ if len(self.variants) >= 2: variant_a, variant_b = self.variants[0], self.variants[1] values_a = self.results[variant_a][metric] values_b = self.results[variant_b][metric] if len(values_a) > 0 and len(values_b) > 0: # tæ£€éªŒ t_stat, p_value = stats.ttest_ind(values_a, values_b) metric_analysis['comparison'] = { 't_statistic': t_stat, 'p_value': p_value, 'significant': p_value &lt; 0.05, 'lift': ( metric_analysis[variant_b]['mean'] - metric_analysis[variant_a]['mean'] ) / metric_analysis[variant_a]['mean'] } analysis[metric] = metric_analysis return analysis def get_winner(self) -> str: """ç¡®å®šè·èƒœå˜ä½“""" analysis = self.analyze() # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©ä¸»è¦æŒ‡æ ‡æœ€é«˜çš„å˜ä½“ primary_metric = self.metrics[0] best_variant = None best_value = float('-inf') for variant in self.variants: if primary_metric in analysis: value = analysis[primary_metric].get(variant, {}).get('mean', float('-inf')) if value > best_value: best_value = value best_variant = variant return best_variant # ä½¿ç”¨ç¤ºä¾‹ def run_ab_test(): """è¿è¡ŒA/Bæµ‹è¯•""" # åˆ›å»ºA/Bæµ‹è¯• ab_test = ABTest( name="fraud_detection_v2", variants=["control", "treatment"], traffic_split={"control": 0.5, "treatment": 0.5}, metrics=["accuracy", "precision", "recall", "f1_score"] ) # åˆ†é…ç”¨æˆ·å¹¶è®°å½•æŒ‡æ ‡ async def process_prediction(user_id: str, prediction: dict, actual: int): """å¤„ç†é¢„æµ‹å¹¶è®°å½•æŒ‡æ ‡""" variant = ab_test.assign_variant(user_id) # ä½¿ç”¨å¯¹åº”å˜ä½“çš„æ¨¡å‹ if variant == "control": result = control_model.predict(prediction['features']) else: result = treatment_model.predict(prediction['features']) # è®¡ç®—æŒ‡æ ‡ accuracy = 1 if result['prediction'] == actual else 0 precision = calculate_precision(result, actual) recall = calculate_recall(result, actual) f1_score = 2 * (precision * recall) / (precision + recall) # è®°å½•æŒ‡æ ‡ ab_test.record_metric(variant, "accuracy", accuracy) ab_test.record_metric(variant, "precision", precision) ab_test.record_metric(variant, "recall", recall) ab_test.record_metric(variant, "f1_score", f1_score) # åˆ†æç»“æœ analysis = ab_test.analyze() print("A/B Test Analysis:") print(analysis) # è·å–è·èƒœå˜ä½“ winner = ab_test.get_winner() print(f"Winner: {winner}") return analysis, winner äº”ã€ç«¯åˆ°ç«¯MLOpsæµæ°´çº¿ 5.1 CI/CDé›†æˆ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 # .github/workflows/mlops-pipeline.yml name: MLOps Pipeline on: push: branches: [main] paths: - 'models/**' - 'data/**' - 'training/**' pull_request: branches: [main] jobs: data-validation: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: '3.9' - name: Install dependencies run: | pip install -r requirements.txt - name: Validate data run: | python scripts/validate_data.py - name: Check data drift run: | python scripts/check_drift.py train-model: needs: data-validation runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Python uses: actions/setup-python@v4 with: python-version: '3.9' - name: Install dependencies run: | pip install -r requirements.txt - name: Train model env: MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }} run: | python scripts/train_model.py - name: Run tests run: | pytest tests/ evaluate-model: needs: train-model runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Evaluate model env: MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }} run: | python scripts/evaluate_model.py - name: Check thresholds run: | python scripts/check_thresholds.py deploy-model: needs: evaluate-model if: github.ref == 'refs/heads/main' runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Deploy to staging run: | kubectl apply -f k8s/staging/ - name: Run smoke tests run: | python scripts/smoke_test.py - name: Promote to production run: | kubectl apply -f k8s/production/ 5.2 å®Œæ•´MLOpsé¡¹ç›®ç»“æ„ mlops-project/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ raw/ # åŸå§‹æ•°æ® â”‚ â”œâ”€â”€ processed/ # å¤„ç†åæ•°æ® â”‚ â””â”€â”€ features/ # ç‰¹å¾æ•°æ® â”œâ”€â”€ models/ â”‚ â”œâ”€â”€ training/ # è®­ç»ƒè„šæœ¬ â”‚ â”‚ â”œâ”€â”€ train.py â”‚ â”‚ â”œâ”€â”€ evaluate.py â”‚ â”‚ â””â”€â”€ tune.py â”‚ â”œâ”€â”€ inference/ # æ¨ç†ä»£ç  â”‚ â”‚ â”œâ”€â”€ predict.py â”‚ â”‚ â””â”€â”€ batch_predict.py â”‚ â””â”€â”€ monitoring/ # ç›‘æ§è„šæœ¬ â”‚ â”œâ”€â”€ drift_detector.py â”‚ â””â”€â”€ performance_monitor.py â”œâ”€â”€ features/ â”‚ â”œâ”€â”€ feature_store.py # ç‰¹å¾å­˜å‚¨ â”‚ â””â”€â”€ feature_registry.py # ç‰¹å¾æ³¨å†Œè¡¨ â”œâ”€â”€ experiments/ â”‚ â””â”€â”€ notebooks/ # å®éªŒç¬”è®°æœ¬ â”œâ”€â”€ tests/ â”‚ â”œâ”€â”€ unit/ â”‚ â”œâ”€â”€ integration/ â”‚ â””â”€â”€ performance/ â”œâ”€â”€ deployment/ â”‚ â”œâ”€â”€ k8s/ # Kubernetesé…ç½® â”‚ â”œâ”€â”€ docker/ # Dockerfile â”‚ â””â”€â”€ terraform/ # åŸºç¡€è®¾æ–½ä»£ç  â”œâ”€â”€ mlflow/ # MLflowé…ç½® â”œâ”€â”€ dvc/ # DVCé…ç½® â”œâ”€â”€ requirements.txt â””â”€â”€ README.md æ€»ç»“ AI/MLç³»ç»Ÿçš„å·¥ç¨‹åŒ–å®è·µéœ€è¦ï¼š
...</p></div><footer class=entry-footer><div class=post-meta-content><div class=post-categories-inline><a href=/blog/categories/ai%E5%BC%80%E5%8F%91/ class=category-link>AIå¼€å‘</a><a href=/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ class=category-link>æœºå™¨å­¦ä¹ </a></div><span class=post-date>2025-12-30</span><span class=post-author>æœ‰æ¡å·¥å…·å›¢é˜Ÿ</span></div><style>.post-meta-content{display:flex;align-items:center;flex-wrap:wrap;gap:.75rem;font-family:sf mono,monaco,courier new,monospace;font-size:.875rem;color:#9ca3af !important}.post-categories-inline{display:inline-flex;align-items:center;gap:.5rem}.category-link{display:inline-flex;align-items:center;gap:.25rem;padding:.25rem .75rem;background:rgba(255,255,255,.1);color:#e5e7eb !important;text-decoration:none;font-size:.75rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;border:1px solid rgba(255,255,255,.2);border-radius:0;transition:all .3s ease;white-space:nowrap}.category-link:hover{background:rgba(255,255,255,.2);color:#fff !important;border-color:rgba(255,255,255,.3);transform:translateY(-1px)}.category-link::before{content:'ğŸ“';font-size:.7rem;opacity:.8}.post-date,.post-reading-time,.post-word-count,.post-author{color:#9ca3af !important}[data-theme=dark] .post-meta-content{color:#9ca3af !important}[data-theme=dark] .category-link{background:rgba(255,255,255,.1);color:#e5e7eb !important;border-color:rgba(255,255,255,.2)}[data-theme=dark] .category-link:hover{background:rgba(255,255,255,.2);color:#fff !important;border-color:rgba(255,255,255,.3)}[data-theme=dark] .post-date,[data-theme=dark] .post-reading-time,[data-theme=dark] .post-word-count,[data-theme=dark] .post-author{color:#9ca3af !important}[data-theme=light] .post-meta-content{color:#6c757d !important}[data-theme=light] .category-link{background:rgba(0,0,0,5%);color:#495057 !important;border-color:rgba(0,0,0,.15)}[data-theme=light] .category-link:hover{background:rgba(0,102,204,.1);color:#212529 !important;border-color:rgba(0,102,204,.3)}[data-theme=light] .post-date,[data-theme=light] .post-reading-time,[data-theme=light] .post-word-count,[data-theme=light] .post-author{color:#6c757d !important}@media(max-width:768px){.post-meta-content{gap:.5rem;font-size:.8rem}.category-link{padding:.2rem .6rem;font-size:.7rem}}</style></footer><a class=entry-link aria-label="post link to AIä¸æœºå™¨å­¦ä¹ å·¥ç¨‹åŒ–å®è·µï¼šä»æ¨¡å‹åˆ°ç”Ÿäº§ç³»ç»Ÿçš„å®Œæ•´æŒ‡å—" href=/blog/articles/ai%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%AE%9E%E8%B7%B5%E4%BB%8E%E6%A8%A1%E5%9E%8B%E5%88%B0%E7%94%9F%E4%BA%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%8C%E6%95%B4%E6%8C%87%E5%8D%97/></a></article></main><footer class=footer><span>Â© 2024-2025 æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢</span> Â·</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>