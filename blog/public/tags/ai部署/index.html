<!doctype html><html lang=zh-cn dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AIéƒ¨ç½² | æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·</title><meta name=keywords content><meta name=description content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"><meta name=author content="util.cn Team"><link rel=canonical href=/blog/tags/ai%E9%83%A8%E7%BD%B2/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.7e8505b7cdf8bb22ab2305e53c2700bb06c7e64faeb72cd3468823a9a3bd3d6e.css integrity="sha256-foUFt834uyKrIwXlPCcAuwbH5k+utyzTRogjqaO9PW4=" rel="preload stylesheet" as=style><link rel=icon href=/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=/blog/favicon-32x32.png><link rel=apple-touch-icon href=/blog/apple-touch-icon.png><link rel=mask-icon href=/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=/blog/tags/ai%E9%83%A8%E7%BD%B2/feed.xml title=rss><link rel=alternate hreflang=zh-cn href=/blog/tags/ai%E9%83%A8%E7%BD%B2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script src=/js/external-link-config.js></script><script src=/js/external-link-interceptor.js></script><link rel=stylesheet href=/blog/css/custom.css media=screen><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","name":"æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«","url":"https://www.util.cn/blog/","description":"æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚æä¾›JSONæ ¼å¼åŒ–ã€SQLä¼˜åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰åœ¨çº¿å·¥å…·çš„è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼Œå¸®åŠ©å¼€å‘è€…æå‡å·¥ä½œæ•ˆç‡ã€‚","publisher":{"@type":"Organization","name":"æœ‰æ¡å·¥å…·","url":"https://www.util.cn","logo":{"@type":"ImageObject","url":"https://www.util.cn/blog/logo/logo-256.png","width":256,"height":256}},"potentialAction":[{"@type":"SearchAction","target":"https://www.util.cn/blog/search?q={search_term_string}","query-input":"required name=search_term_string"}]}</script><meta property="og:type" content="website"><meta property="og:title" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«"><meta property="og:description" content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚æä¾›JSONæ ¼å¼åŒ–ã€SQLä¼˜åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰åœ¨çº¿å·¥å…·çš„è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼Œå¸®åŠ©å¼€å‘è€…æå‡å·¥ä½œæ•ˆç‡ã€‚"><meta property="og:url" content="https://www.util.cn/blog/"><meta property="og:site_name" content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢"><meta property="og:image" content="https://www.util.cn/blog/logo/logo-256.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· | å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ & ç¼–ç¨‹æŠ€å·§åˆ†äº«"><meta name=twitter:description content="æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ - åˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒã€‚"><meta name=twitter:image content="https://www.util.cn/blog/logo/logo-256.png"><meta name=baidu-site-verification content><meta name=category content="æŠ€æœ¯åšå®¢,å¼€å‘è€…å·¥å…·,ç¼–ç¨‹æ•™ç¨‹"><meta name=coverage content="Worldwide"><meta name=distribution content="Global"><meta name=rating content="General"><script id=51la_code async crossorigin=anonymous src="https://sdk.51.la/js-sdk-pro.min.js?id=3OM52V0xJAPv6ozF&hash=pro"></script><script>window.addEventListener("load",function(){setTimeout(function(){typeof la!="undefined"?console.log("51laç»Ÿè®¡å·²åŠ è½½"):(console.warn("51laç»Ÿè®¡åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ"),function(){var e=document.createElement("script");e.src="https://sdk.51.la/js-sdk-pro.min.js?id=3OM52V0xJAPv6ozF&hash=backup",e.async=!0,document.head.appendChild(e)}())},3e3)})</script><meta property="og:url" content="/blog/tags/ai%E9%83%A8%E7%BD%B2/"><meta property="og:site_name" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·"><meta property="og:title" content="AIéƒ¨ç½²"><meta property="og:description" content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="AIéƒ¨ç½²"><meta name=twitter:description content="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·ï¼šåˆ†äº«å¼€å‘è€…å·¥å…·ä½¿ç”¨æ•™ç¨‹ã€ç¼–ç¨‹æŠ€å·§å’Œå®æˆ˜å¼€å‘ç»éªŒ"></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=/blog/ accesskey=h title="æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…· (Alt + H)">æŠ€æœ¯åšå®¢ - æœ‰æ¡å·¥å…·</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/blog/posts/ title="æ‰€æœ‰æ–‡ç« åˆ—è¡¨ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŸ¥çœ‹æ‰€æœ‰æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹"><span>æ–‡ç« </span></a></li><li><a href=https://www.util.cn title="æœ‰æ¡å·¥å…· - å¼€å‘è€…çš„å¸¸ç”¨å·¥å…·é›†åˆï¼šæ— å¹¿å‘Š Â· æœ¬åœ°è®¡ç®— Â· å³å¼€å³ç”¨çš„åœ¨çº¿å·¥å…·å¹³å°ï¼Œæä¾›JSONæ ¼å¼åŒ–ã€SQLæ ¼å¼åŒ–ã€Markdownç¼–è¾‘å™¨ç­‰å®ç”¨å·¥å…·"><span>æœ‰æ¡å·¥å…·</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=/blog/categories/ title="æ–‡ç« åˆ†ç±» - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŒ‰æŠ€æœ¯é¢†åŸŸåˆ†ç±»çš„ä¼˜è´¨æ–‡ç« ï¼ŒåŒ…æ‹¬å‰ç«¯å¼€å‘ã€å·¥å…·ä½¿ç”¨ã€ç¼–ç¨‹æŠ€å·§ç­‰"><span>åˆ†ç±»</span></a></li><li><a href=/blog/tags/ title="æ ‡ç­¾äº‘ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šé€šè¿‡æ ‡ç­¾å¿«é€Ÿæ‰¾åˆ°æ„Ÿå…´è¶£çš„æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹"><span>æ ‡ç­¾</span></a></li><li><a href=/blog/archives/ title="æ–‡ç« å½’æ¡£ - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šæŒ‰æ—¶é—´æŸ¥çœ‹å†å²æ–‡ç« "><span>å½’æ¡£</span></a></li><li><a href=/blog/search/ title="æœç´¢æ–‡ç«  - æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢ï¼šé€šè¿‡å…³é”®è¯æœç´¢æ‰¾åˆ°æ„Ÿå…´è¶£çš„æŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹å†…å®¹ï¼Œæ”¯æŒæ ‡é¢˜ã€å†…å®¹ã€åˆ†ç±»å’Œæ ‡ç­¾æœç´¢"><span>æœç´¢</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>AIéƒ¨ç½²</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Edge AIæ¶æ„è®¾è®¡ï¼šåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ™ºèƒ½åº”ç”¨</h2></header><div class=entry-content><p>å¼•è¨€ éšç€AIèŠ¯ç‰‡çš„æ™®åŠå’Œæ¨¡å‹ä¼˜åŒ–æŠ€æœ¯çš„è¿›æ­¥ï¼Œ2025å¹´Edge AIè¿æ¥äº†çˆ†å‘å¼å¢é•¿ã€‚ä»æ™ºèƒ½æ‰‹æœºåˆ°å·¥ä¸šè®¾å¤‡ï¼Œæ™ºèƒ½æ­£åœ¨ä»äº‘ç«¯ä¸‹æ²‰åˆ°è¾¹ç¼˜ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Edge AIçš„æ¶æ„è®¾è®¡ï¼Œå¸®åŠ©å¼€å‘è€…åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ€§èƒ½AIåº”ç”¨ã€‚
ä¸€ã€Edge AIæŠ€æœ¯æ ˆ 1.1 æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 # æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ– import tensorflow as tf import tensorflow_model_optimization as tfmot from tensorflow.lite.python import converter import numpy as np class ModelOptimizer: """è¾¹ç¼˜AIæ¨¡å‹ä¼˜åŒ–å™¨""" def __init__(self, model): self.original_model = model self.optimized_model = None def quantize_model(self, model, representative_data=None): """ æ¨¡å‹é‡åŒ–ï¼šå°†32ä½æµ®ç‚¹æ•°è½¬æ¢ä¸º8ä½æ•´æ•° - æ¨¡å‹å¤§å°å‡å°‘75% - æ¨ç†é€Ÿåº¦æå‡2-4å€ - ç²¾åº¦æŸå¤±é€šå¸¸&lt;1% """ if representative_data is None: # è®­ç»ƒåé‡åŒ–ï¼ˆæ— éœ€æ ¡å‡†æ•°æ®ï¼‰ converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] else: # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆéœ€è¦æ ¡å‡†æ•°æ®ï¼‰ def representative_dataset(): for data in representative_data: yield [data] converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.representative_dataset = representative_dataset converter.target_spec.supported_types = [tf.float16] # å¯é€‰ï¼šfloat16 tflite_model = converter.convert() return tflite_model def prune_model(self, model, pruning_params=None): """ æ¨¡å‹å‰ªæï¼šç§»é™¤ä¸é‡è¦çš„è¿æ¥ - å‡å°‘æ¨¡å‹å¤æ‚åº¦ - åŠ é€Ÿæ¨ç† - é˜²æ­¢è¿‡æ‹Ÿåˆ """ if pruning_params is None: pruning_params = { 'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay( initial_sparsity=0.0, final_sparsity=0.5, # å‰ªæ50% begin_step=0, end_step=1000 ), 'block_size': (1, 1), # ç»“æ„åŒ–å‰ªæ } # åº”ç”¨å‰ªæ pruning_model = tfmot.sparsity.keras.prune_low_magnitude( model, **pruning_params ) # å¾®è°ƒå‰ªæåçš„æ¨¡å‹ pruning_model.compile( optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # pruning_model.fit(x_train, y_train, epochs=3) return pruning_model def distill_model(self, teacher_model, student_model, data): """ çŸ¥è¯†è’¸é¦ï¼šç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹ - ä¿ç•™å¤§æ¨¡å‹çš„çŸ¥è¯† - å¤§å¹…å‡å°æ¨¡å‹å¤§å° """ # æ¸©åº¦å‚æ•°ï¼šæ§åˆ¶è¾“å‡ºåˆ†å¸ƒçš„å¹³æ»‘åº¦ temperature = 3 # æ•™å¸ˆæ¨¡å‹çš„è½¯æ ‡ç­¾ teacher_logits = teacher_model.predict(data, verbose=0) # å­¦ç”Ÿæ¨¡å‹å­¦ä¹ è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾ def distillation_loss(y_true, y_pred): # è½¯æ ‡ç­¾æŸå¤± soft_loss = tf.keras.losses.KLDivergence()( tf.nn.softmax(teacher_logits / temperature), tf.nn.softmax(y_pred / temperature) ) # ç¡¬æ ‡ç­¾æŸå¤± hard_loss = tf.keras.losses.sparse_categorical_crossentropy( y_true, y_pred ) # åŠ æƒç»„åˆ return 0.7 * soft_loss * temperature ** 2 + 0.3 * hard_loss student_model.compile( optimizer='adam', loss=distillation_loss, metrics=['accuracy'] ) return student_model def optimize_for_hardware(self, tflite_model, target_hardware): """ é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ– """ # Edge TPUä¼˜åŒ– if target_hardware == 'edge_tpu': # Edge TPUåªæ”¯æŒ8ä½æ•´æ•°é‡åŒ– converter = tf.lite.TFLiteConverter.from_keras_model(self.original_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] converter.inference_input_type = tf.uint8 converter.inference_output_type = tf.uint8 # GPUä¼˜åŒ– elif target_hardware == 'gpu': converter = tf.lite.TFLiteConverter.from_keras_model(self.original_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] converter.target_spec.supported_types = [tf.float16] # é€šç”¨CPUä¼˜åŒ– else: converter = tf.lite.TFLiteConverter.from_keras_model(self.original_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] return converter.convert() # å®é™…ä½¿ç”¨ç¤ºä¾‹ optimizer = ModelOptimizer(original_model) # 1. é‡åŒ–æ¨¡å‹ quantized_model = optimizer.quantize_model( original_model, representative_data=calibration_data ) # 2. ä¿å­˜æ¨¡å‹ with open('model_quant.tflite', 'wb') as f: f.write(quantized_model) # 3. æŸ¥çœ‹æ¨¡å‹å¤§å° import os original_size = os.path.getsize('model.h5') / (1024 * 1024) # MB quantized_size = os.path.getsize('model_quant.tflite') / (1024 * 1024) print(f"åŸå§‹æ¨¡å‹: {original_size:.2f} MB") print(f"é‡åŒ–æ¨¡å‹: {quantized_size:.2f} MB") print(f"å‹ç¼©ç‡: {(1 - quantized_size / original_size) * 100:.1f}%") 1.2 è¾¹ç¼˜æ¨ç†æ¡†æ¶ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 # TensorFlow Liteéƒ¨ç½² class TFLiteInferenceEngine: """TensorFlow Liteæ¨ç†å¼•æ“""" def __init__(self, model_path, num_threads=4, use_gpu=False): import tflite_runtime.interpreter as tflite # åŠ è½½æ¨¡å‹ self.interpreter = tflite.Interpreter( model_path=model_model_path, num_threads=num_threads ) # åˆ†é…å¼ é‡ self.interpreter.allocate_tensors() # è·å–è¾“å…¥è¾“å‡ºè¯¦æƒ… self.input_details = self.interpreter.get_input_details() self.output_details = self.interpreter.get_output_details() # æ£€æŸ¥ç¡¬ä»¶åŠ é€Ÿ if use_gpu: self._enable_gpu_delegation() def _enable_gpu_delegation(self): """å¯ç”¨GPUåŠ é€Ÿ""" import tflite_runtime.interpreter as tflite # GPUå§”æ‰˜ options = tflite.InterpreterOptions() options.add_delegate("TfLiteGpuDelegateV2") self.interpreter = tflite.Interpreter( model_path=self.model_path, interpreter_options=options ) def predict(self, input_data): """æ‰§è¡Œæ¨ç†""" # è®¾ç½®è¾“å…¥ input_index = self.input_details[0]['index'] self.interpreter.set_tensor(input_index, input_data) # æ‰§è¡Œæ¨ç† self.interpreter.invoke() # è·å–è¾“å‡º output_index = self.output_details[0]['index'] output_data = self.interpreter.get_tensor(output_index) return output_data def predict_batch(self, input_batch): """æ‰¹é‡æ¨ç†""" results = [] for input_data in input_batch: result = self.predict(input_data) results.append(result) return np.array(results) # ONNX Runtimeéƒ¨ç½² class ONNXInferenceEngine: """ONNX Runtimeæ¨ç†å¼•æ“""" def __init__(self, model_path, providers=None): import onnxruntime as ort if providers is None: # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ‰§è¡Œæä¾›è€… providers = ort.get_available_providers() self.session = ort.InferenceSession( model_path, providers=providers ) # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯ self.input_name = self.session.get_inputs()[0].name self.output_name = self.session.get_outputs()[0].name def predict(self, input_data): """æ‰§è¡Œæ¨ç†""" result = self.session.run( [self.output_name], {self.input_name: input_data} ) return result[0] def get_model_info(self): """è·å–æ¨¡å‹ä¿¡æ¯""" return { 'inputs': [{ 'name': input.name, 'shape': input.shape, 'type': input.type } for input in self.session.get_inputs() ], 'outputs': [{ 'name': output.name, 'shape': output.shape, 'type': output.type } for output in self.session.get_outputs() ], 'providers': self.session.get_providers() } # PyTorch Mobileéƒ¨ç½² class PyTorchMobileEngine: """PyTorch Mobileæ¨ç†å¼•æ“""" def __init__(self, model_path): import torch import torch.jit as jit # åŠ è½½TorchScriptæ¨¡å‹ self.model = jit.load(model_path) self.model.eval() # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰ if torch.cuda.is_available(): self.model = self.model.cuda() def predict(self, input_data): import torch # è½¬æ¢ä¸ºtensor if not isinstance(input_data, torch.Tensor): input_tensor = torch.tensor(input_data) else: input_tensor = input_data # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœæ¨¡å‹åœ¨GPUï¼‰ if torch.cuda.is_available() and next(self.model.parameters()).is_cuda: input_tensor = input_tensor.cuda() # æ¨ç† with torch.no_grad(): output = self.model(input_tensor) # ç§»å›CPUå¹¶è½¬æ¢ä¸ºnumpy return output.cpu().numpy() äºŒã€ç«¯äº‘ååŒæ¶æ„ 2.1 ååŒæ¨ç†æ¨¡å¼ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 # ç«¯äº‘ååŒæ¨ç† class EdgeCloudCollaborativeInference: """ç«¯äº‘ååŒæ¨ç†""" def __init__(self, edge_model, cloud_api): self.edge_model = edge_model # è½»é‡çº§è¾¹ç¼˜æ¨¡å‹ self.cloud_api = cloud_api # äº‘ç«¯API # é˜ˆå€¼é…ç½® self.confidence_threshold = 0.8 self.latency_threshold = 100 # ms self.bandwidth_threshold = 100 # KB/s async def predict(self, input_data): """æ™ºèƒ½æ¨ç†å†³ç­–""" # å°è¯•è¾¹ç¼˜æ¨ç† edge_result = await self._edge_inference(input_data) # è¯„ä¼°è¾¹ç¼˜ç»“æœ if self._should_use_edge_result(edge_result): return edge_result else: # å›é€€åˆ°äº‘ç«¯ return await self._cloud_inference(input_data) async def _edge_inference(self, input_data): """è¾¹ç¼˜æ¨ç†""" start_time = time.time() # æœ¬åœ°æ¨ç† result = self.edge_model.predict(input_data) latency = (time.time() - start_time) * 1000 # ms return { 'result': result, 'confidence': result.get('confidence', 0.5), 'latency': latency, 'source': 'edge' } async def _cloud_inference(self, input_data): """äº‘ç«¯æ¨ç†""" start_time = time.time() # è°ƒç”¨äº‘ç«¯API cloud_result = await self.cloud_api.predict(input_data) latency = (time.time() - start_time) * 1000 # ms return { 'result': cloud_result, 'confidence': cloud_result.get('confidence', 0.9), 'latency': latency, 'source': 'cloud' } def _should_use_edge_result(self, edge_result): """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨è¾¹ç¼˜ç»“æœ""" # é«˜ç½®ä¿¡åº¦ï¼šä½¿ç”¨è¾¹ç¼˜ç»“æœ if edge_result['confidence'] >= self.confidence_threshold: return True # ä½å»¶è¿Ÿè¦æ±‚ï¼šä½¿ç”¨è¾¹ç¼˜ç»“æœ if edge_result['latency'] &lt;= self.latency_threshold: return True # ä½å¸¦å®½ç¯å¢ƒï¼šä½¿ç”¨è¾¹ç¼˜ç»“æœ if self._get_bandwidth() &lt; self.bandwidth_threshold: return True return False def _get_bandwidth(self): """è·å–å½“å‰å¸¦å®½""" # ç®€åŒ–å®ç° return 1000 # KB/s # 2. åˆ†å±‚æ¨ç†æ¨¡å‹ class HierarchicalInference: """åˆ†å±‚æ¨ç†ï¼šè¾¹ç¼˜é¢„ç­›é€‰+äº‘ç«¯ç²¾ç»†å¤„ç†""" def __init__(self, edge_filter, cloud_processor): self.edge_filter = edge_filter # è½»é‡çº§è¿‡æ»¤å™¨ self.cloud_processor = cloud_processor # é‡é‡çº§å¤„ç†å™¨ async def process(self, input_data): """åˆ†å±‚å¤„ç†""" # ç¬¬ä¸€å±‚ï¼šè¾¹ç¼˜å¿«é€Ÿè¿‡æ»¤ filter_result = await self._filter_on_edge(input_data) if filter_result['is_simple']: # ç®€å•æ¡ˆä¾‹ï¼šç›´æ¥è¿”å› return filter_result['result'] else: # å¤æ‚æ¡ˆä¾‹ï¼šäº‘ç«¯æ·±åº¦å¤„ç† return await self._process_on_cloud(input_data, filter_result) async def _filter_on_edge(self, input_data): """è¾¹ç¼˜è¿‡æ»¤""" # å¿«é€Ÿåˆ†ç±» category = self.edge_filter.predict(input_data) return { 'is_simple': category == 'simple', 'result': category, 'confidence': 0.9 } async def _process_on_cloud(self, input_data, filter_result): """äº‘ç«¯æ·±åº¦å¤„ç†""" # å°†è¾¹ç¼˜è¿‡æ»¤ç»“æœä¼ é€’ç»™äº‘ç«¯ cloud_result = await self.cloud_processor.process( input_data, context=filter_result ) return cloud_result # 3. å¢é‡å­¦ä¹ æ¶æ„ class IncrementalLearningEdge: """è¾¹ç¼˜å¢é‡å­¦ä¹ """ def __init__(self, base_model): self.base_model = base_model self.local_adaptations = {} async def predict_with_adaptation(self, input_data, user_id): """å¸¦æœ¬åœ°é€‚é…çš„é¢„æµ‹""" # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ç‰¹å®šé€‚é… if user_id in self.local_adaptations: adapted_model = self.local_adaptations[user_id] result = adapted_model.predict(input_data) else: result = self.base_model.predict(input_data) return result async def update_local_model(self, user_id, new_data): """æ›´æ–°æœ¬åœ°æ¨¡å‹""" # åŸºäºæ–°æ•°æ®å¾®è°ƒæ¨¡å‹ adapted_model = self._fine_tune_model( self.base_model, new_data ) self.local_adaptations[user_id] = adapted_model # å®šæœŸåŒæ­¥åˆ°äº‘ç«¯ await self._sync_to_cloud(user_id, adapted_model) def _fine_tune_model(self, base_model, data): """å¾®è°ƒæ¨¡å‹""" # ç®€åŒ–å®ç°ï¼šè¿ç§»å­¦ä¹  # å®é™…åº”ä½¿ç”¨æ›´å¤æ‚çš„å¾®è°ƒç­–ç•¥ return base_model # è¿”å›é€‚é…åçš„æ¨¡å‹ 2.2 ç¦»çº¿æ¨ç†æ¶æ„ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 # ç¦»çº¿AIæ¨ç† class OfflineInferenceEngine: """ç¦»çº¿æ¨ç†å¼•æ“""" def __init__(self, model_path, cache_size=100): self.model = self._load_model(model_path) self.cache = LRUCache(cache_size) self.offline_models = {} def _load_model(self, model_path): """åŠ è½½æ¨¡å‹""" # TFLiteåŠ è½½ import tflite_runtime.interpreter as tflite return tflite.Interpreter(model_path=model_path) async def predict(self, input_data, model_version='default'): """ç¦»çº¿é¢„æµ‹""" # æ£€æŸ¥ç¼“å­˜ cache_key = self._generate_cache_key(input_data) cached_result = self.cache.get(cache_key) if cached_result: return cached_result # æ‰§è¡Œæ¨ç† result = self._do_inference(input_data, model_version) # ç¼“å­˜ç»“æœ self.cache.put(cache_key, result) return result def _do_inference(self, input_data, model_version): """æ‰§è¡Œæ¨ç†""" # ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬æ¨¡å‹ if model_version != 'default': model = self.offline_models.get(model_version) if model: return model.predict(input_data) # ä½¿ç”¨é»˜è®¤æ¨¡å‹ return self._predict_with_model(self.model, input_data) def download_model(self, model_url, version): """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°""" # ä¸‹è½½æ¨¡å‹æ–‡ä»¶ model_data = self._download_from_url(model_url) # ä¿å­˜åˆ°æœ¬åœ° model_path = f"/models/{version}.tflite" with open(model_path, 'wb') as f: f.write(model_data) # åŠ è½½æ¨¡å‹ import tflite_runtime.interpreter as tflite model = tflite.Interpreter(model_path=model_path) self.offline_models[version] = model return model def update_model(self, new_version): """æ›´æ–°æ¨¡å‹""" # æ£€æŸ¥ç½‘ç»œè¿æ¥ if not self._is_online(): raise Exception("Offline: Cannot update model") # ä¸‹è½½æ–°ç‰ˆæœ¬ model_url = f"https://api.example.com/models/{new_version}.tflite" self.download_model(model_url, new_version) # è®¾ç½®ä¸ºé»˜è®¤ç‰ˆæœ¬ self.model = self.offline_models[new_version] def _is_online(self): """æ£€æŸ¥ç½‘ç»œè¿æ¥""" import socket try: socket.create_connection(("8.8.8.8", 53), timeout=3) return True except OSError: return False class LRUCache: """LRUç¼“å­˜""" def __init__(self, size): self.size = size self.cache = {} self.order = [] def get(self, key): if key in self.cache: # æ›´æ–°è®¿é—®é¡ºåº self.order.remove(key) self.order.append(key) return self.cache[key] return None def put(self, key, value): if key in self.cache: self.order.remove(key) elif len(self.cache) >= self.size: # ç§»é™¤æœ€æ—§çš„é¡¹ oldest = self.order.pop(0) del self.cache[oldest] self.cache[key] = value self.order.append(key) ä¸‰ã€å®æ—¶æ€§ä¸èƒ½æ•ˆä¼˜åŒ– 3.1 å®æ—¶æ€§ä¼˜åŒ– 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 # å®æ—¶AIæ¨ç†ä¼˜åŒ– class RealtimeInferenceOptimizer: """å®æ—¶æ¨ç†ä¼˜åŒ–å™¨""" def __init__(self, model): self.model = model self.input_queue = queue.Queue(maxsize=10) self.output_queue = queue.Queue(maxsize=10) async def start_inference_thread(self): """å¯åŠ¨æ¨ç†çº¿ç¨‹""" import threading thread = threading.Thread(target=self._inference_loop) thread.daemon = True thread.start() def _inference_loop(self): """æ¨ç†å¾ªç¯""" while True: try: # ä»é˜Ÿåˆ—è·å–è¾“å…¥ input_data = self.input_queue.get(timeout=1.0) # æ‰§è¡Œæ¨ç† result = self.model.predict(input_data) # æ”¾å…¥è¾“å‡ºé˜Ÿåˆ— self.output_queue.put(result) except queue.Empty: continue async def predict_async(self, input_data): """å¼‚æ­¥æ¨ç†""" # éé˜»å¡æäº¤ try: self.input_queue.put_nowait(input_data) except queue.Full: raise Exception("Inference queue full") # ç­‰å¾…ç»“æœï¼ˆå¸¦è¶…æ—¶ï¼‰ try: result = self.output_queue.get(timeout=0.1) return result except queue.Empty: raise Exception("Inference timeout") # 2. æ¨¡å‹åˆ†ç‰‡ class ModelSharding: """æ¨¡å‹åˆ†ç‰‡ï¼šå°†å¤§æ¨¡å‹æ‹†åˆ†ä¸ºå°æ¨¡å—""" def __init__(self, model_config): self.shards = {} self._create_shards(model_config) def _create_shards(self, config): """åˆ›å»ºæ¨¡å‹åˆ†ç‰‡""" for shard_name, shard_config in config['shards'].items(): self.shards[shard_name] = self._load_shard(shard_config) def _load_shard(self, config): """åŠ è½½æ¨¡å‹åˆ†ç‰‡""" # åŠ è½½è½»é‡çº§åˆ†ç‰‡æ¨¡å‹ return load_model(config['path']) def predict_sharded(self, input_data, shard_order): """åˆ†ç‰‡æ¨ç†""" current_data = input_data for shard_name in shard_order: shard = self.shards[shard_name] current_data = shard.predict(current_data) return current_data # 3. æ—©æœŸé€€å‡º class EarlyExitInference: """æ—©æœŸé€€å‡ºæ¨ç†""" def __init__(self, model): self.model = model self.exit_thresholds = [0.95, 0.8, 0.6] def predict_with_early_exit(self, input_data): """å¸¦æ—©æœŸé€€å‡ºçš„é¢„æµ‹""" # ç¬¬ä¸€å±‚ï¼šå¿«é€Ÿåˆ†ç±» result = self.model.predict_layer1(input_data) confidence = result['confidence'] if confidence > self.exit_thresholds[0]: return result # ç¬¬äºŒå±‚ï¼šç²¾ç»†åˆ†ç±» result = self.model.predict_layer2(input_data) confidence = result['confidence'] if confidence > self.exit_thresholds[1]: return result # ç¬¬ä¸‰å±‚ï¼šå®Œæ•´æ¨ç† result = self.model.predict_full(input_data) return result 3.2 èƒ½æ•ˆä¼˜åŒ– 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 # èƒ½æ•ˆä¼˜åŒ–ç­–ç•¥ class PowerOptimizedInference: """åŠŸè€—ä¼˜åŒ–æ¨ç†""" def __init__(self, model): self.model = model self.power_modes = ['high_performance', 'balanced', 'power_saver'] self.current_mode = 'balanced' def set_power_mode(self, mode): """è®¾ç½®åŠŸè€—æ¨¡å¼""" if mode not in self.power_modes: raise ValueError(f"Invalid power mode: {mode}") self.current_mode = mode # è°ƒæ•´æ¨ç†å‚æ•° if mode == 'high_performance': self.threads = 4 self.precision = 'float32' elif mode == 'balanced': self.threads = 2 self.precision = 'float16' else: # power_saver self.threads = 1 self.precision = 'int8' def predict(self, input_data): """æ ¹æ®åŠŸè€—æ¨¡å¼æ¨ç†""" if self.current_mode == 'power_saver': # é™ä½é¢‘ç‡ self._throttle_inference() result = self._do_inference(input_data) self._restore_inference() else: result = self._do_inference(input_data) return result def _throttle_inference(self): """é™åˆ¶æ¨ç†æ€§èƒ½""" # é™ä½CPUé¢‘ç‡ç­‰ pass def _restore_inference(self): """æ¢å¤æ¨ç†æ€§èƒ½""" pass # 2. æ‰¹å¤„ç†ä¼˜åŒ– class BatchOptimizer: """æ‰¹å¤„ç†ä¼˜åŒ–""" def __init__(self, model, max_batch_size=8, max_wait_time=50): self.model = model self.max_batch_size = max_batch_size self.max_wait_time = max_wait_time # ms self.pending_requests = [] self.last_batch_time = time.time() async def predict(self, input_data): """æ‰¹å¤„ç†æ¨ç†""" # æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ— future = asyncio.Future() self.pending_requests.append({ 'input': input_data, 'future': future }) # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œæ‰¹å¤„ç† if self._should_process_batch(): await self._process_batch() return await future def _should_process_batch(self): """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†æ‰¹æ¬¡""" # è¾¾åˆ°æœ€å¤§æ‰¹å¤§å° if len(self.pending_requests) >= self.max_batch_size: return True # è¶…è¿‡æœ€å¤§ç­‰å¾…æ—¶é—´ elapsed = (time.time() - self.last_batch_time) * 1000 if elapsed >= self.max_wait_time: return True return False async def _process_batch(self): """å¤„ç†æ‰¹æ¬¡""" if not self.pending_requests: return # å‡†å¤‡æ‰¹æ¬¡æ•°æ® batch_inputs = [req['input'] for req in self.pending_requests] # æ‰¹é‡æ¨ç† batch_results = await self._batch_predict(batch_inputs) # è¿”å›ç»“æœ for i, req in enumerate(self.pending_requests): req['future'].set_result(batch_results[i]) # æ¸…ç©ºé˜Ÿåˆ— self.pending_requests = [] self.last_batch_time = time.time() async def _batch_predict(self, batch_inputs): """æ‰¹é‡é¢„æµ‹""" # å †å ä¸ºæ‰¹æ¬¡tensor batch_tensor = np.stack(batch_inputs) # æ¨ç† batch_results = self.model.predict(batch_tensor) return batch_results å››ã€å®é™…åº”ç”¨æ¡ˆä¾‹ 4.1 æ™ºèƒ½ç›¸æœºåº”ç”¨ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 # æ™ºèƒ½ç›¸æœºï¼šè¾¹ç¼˜AIå®æ—¶å›¾åƒå¤„ç† class SmartCameraApp: """æ™ºèƒ½ç›¸æœºåº”ç”¨""" def __init__(self): # åŠ è½½å¤šä¸ªæ¨¡å‹ self.face_detector = self._load_model('face_detector.tflite') self.face_recognizer = self._load_model('face_recognizer.tflite') self.scene_classifier = self._load_model('scene_classifier.tflite') self.image_enhancer = self._load_model('image_enhancer.tflite') async def process_camera_frame(self, frame): """å¤„ç†ç›¸æœºå¸§""" # 1. åœºæ™¯æ£€æµ‹ï¼ˆå¿«é€Ÿï¼‰ scene = self.scene_classifier.predict(frame) # 2. æ ¹æ®åœºæ™¯é€‰æ‹©å¤„ç†æµç¨‹ if scene['class'] == 'portrait': return await self._process_portrait(frame) elif scene['class'] == 'landscape': return await self._process_landscape(frame) elif scene['class'] == 'night': return await self._process_night_scene(frame) else: return await self._process_default(frame) async def _process_portrait(self, frame): """å¤„ç†äººåƒåœºæ™¯""" # æ£€æµ‹äººè„¸ faces = self.face_detector.predict(frame) if faces: # è¯†åˆ«äººè„¸ identities = self.face_recognizer.predict(frame, faces) # ç¾é¢œå¤„ç† enhanced_frame = self.image_enhancer.enance_portrait(frame, faces) return { 'frame': enhanced_frame, 'faces': faces, 'identities': identities, 'effects': ['beautify', 'smooth'] } return {'frame': frame, 'faces': []} async def _process_night_scene(self, frame): """å¤„ç†å¤œæ™¯""" # å¤œæ™¯å¢å¼º enhanced_frame = self.image_enhancer.enforce_night(frame) return { 'frame': enhanced_frame, 'effects': ['night_mode', 'denoise'] } # å®æ—¶æ€§èƒ½ç›‘æ§ class PerformanceMonitor: """æ€§èƒ½ç›‘æ§""" def __init__(self): self.fps_history = [] self.latency_history = [] def measure_inference(self, inference_fn): """æµ‹é‡æ¨ç†æ€§èƒ½""" start_time = time.time() result = inference_fn() end_time = time.time() latency = (end_time - start_time) * 1000 # ms self.latency_history.append(latency) # ä¿æŒæœ€è¿‘100ä¸ªæ ·æœ¬ if len(self.latency_history) > 100: self.latency_history.pop(0) return result def get_stats(self): """è·å–ç»Ÿè®¡ä¿¡æ¯""" if not self.latency_history: return {} import numpy as np return { 'avg_latency': np.mean(self.latency_history), 'p50_latency': np.percentile(self.latency_history, 50), 'p95_latency': np.percentile(self.latency_history, 95), 'p99_latency': np.percentile(self.latency_history, 99), 'max_latency': np.max(self.latency_history), 'fps': 1000 / np.mean(self.latency_history) } æ€»ç»“ Edge AIåœ¨2025å¹´å®ç°äº†ä»æ¦‚å¿µåˆ°å•†ç”¨çš„è·¨è¶Šã€‚2026å¹´ï¼Œéšç€ç¡¬ä»¶èƒ½åŠ›çš„æå‡å’Œä¼˜åŒ–æŠ€æœ¯çš„æˆç†Ÿï¼ŒEdge AIå°†åœ¨æ›´å¤šåœºæ™¯ä¸­æ›¿ä»£äº‘ç«¯AIã€‚
...</p></div><footer class=entry-footer><div class=post-meta-content><div class=post-categories-inline><a href=/blog/categories/ai/ class=category-link>AI</a><a href=/blog/categories/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/ class=category-link>è¾¹ç¼˜è®¡ç®—</a></div><span class=post-date>2025-12-31</span><span class=post-author>æœ‰æ¡å·¥å…·å›¢é˜Ÿ</span></div><style>.post-meta-content{display:flex;align-items:center;flex-wrap:wrap;gap:.75rem;font-family:sf mono,monaco,courier new,monospace;font-size:.875rem;color:#9ca3af !important}.post-categories-inline{display:inline-flex;align-items:center;gap:.5rem}.category-link{display:inline-flex;align-items:center;gap:.25rem;padding:.25rem .75rem;background:rgba(255,255,255,.1);color:#e5e7eb !important;text-decoration:none;font-size:.75rem;font-weight:600;text-transform:uppercase;letter-spacing:.05em;border:1px solid rgba(255,255,255,.2);border-radius:0;transition:all .3s ease;white-space:nowrap}.category-link:hover{background:rgba(255,255,255,.2);color:#fff !important;border-color:rgba(255,255,255,.3);transform:translateY(-1px)}.category-link::before{content:'ğŸ“';font-size:.7rem;opacity:.8}.post-date,.post-reading-time,.post-word-count,.post-author{color:#9ca3af !important}[data-theme=dark] .post-meta-content{color:#9ca3af !important}[data-theme=dark] .category-link{background:rgba(255,255,255,.1);color:#e5e7eb !important;border-color:rgba(255,255,255,.2)}[data-theme=dark] .category-link:hover{background:rgba(255,255,255,.2);color:#fff !important;border-color:rgba(255,255,255,.3)}[data-theme=dark] .post-date,[data-theme=dark] .post-reading-time,[data-theme=dark] .post-word-count,[data-theme=dark] .post-author{color:#9ca3af !important}[data-theme=light] .post-meta-content{color:#6c757d !important}[data-theme=light] .category-link{background:rgba(0,0,0,5%);color:#495057 !important;border-color:rgba(0,0,0,.15)}[data-theme=light] .category-link:hover{background:rgba(0,102,204,.1);color:#212529 !important;border-color:rgba(0,102,204,.3)}[data-theme=light] .post-date,[data-theme=light] .post-reading-time,[data-theme=light] .post-word-count,[data-theme=light] .post-author{color:#6c757d !important}@media(max-width:768px){.post-meta-content{gap:.5rem;font-size:.8rem}.category-link{padding:.2rem .6rem;font-size:.7rem}}</style></footer><a class=entry-link aria-label="post link to Edge AIæ¶æ„è®¾è®¡ï¼šåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ™ºèƒ½åº”ç”¨" href=/blog/articles/edge-ai%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%9C%A8%E8%B5%84%E6%BA%90%E5%8F%97%E9%99%90%E8%AE%BE%E5%A4%87%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%99%BA%E8%83%BD%E5%BA%94%E7%94%A8/></a></article></main><footer class=footer><span>Â© 2024-2025 æœ‰æ¡å·¥å…·æŠ€æœ¯åšå®¢</span> Â·</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>